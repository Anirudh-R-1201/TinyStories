transformer.wte.weight torch.Size([50257, 256])
transformer.wpe.weight torch.Size([2048, 256])
transformer.h.0.ln_1.weight torch.Size([256])
transformer.h.0.ln_1.bias torch.Size([256])
transformer.h.0.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.0.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.0.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.0.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.0.attn.attention.out_proj.bias torch.Size([256])
transformer.h.0.ln_2.weight torch.Size([256])
transformer.h.0.ln_2.bias torch.Size([256])
transformer.h.0.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.0.mlp.c_fc.bias torch.Size([1024])
transformer.h.0.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.0.mlp.c_proj.bias torch.Size([256])
transformer.h.1.ln_1.weight torch.Size([256])
transformer.h.1.ln_1.bias torch.Size([256])
transformer.h.1.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.1.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.1.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.1.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.1.attn.attention.out_proj.bias torch.Size([256])
transformer.h.1.ln_2.weight torch.Size([256])
transformer.h.1.ln_2.bias torch.Size([256])
transformer.h.1.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.1.mlp.c_fc.bias torch.Size([1024])
transformer.h.1.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.1.mlp.c_proj.bias torch.Size([256])
transformer.h.2.ln_1.weight torch.Size([256])
transformer.h.2.ln_1.bias torch.Size([256])
transformer.h.2.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.2.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.2.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.2.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.2.attn.attention.out_proj.bias torch.Size([256])
transformer.h.2.ln_2.weight torch.Size([256])
transformer.h.2.ln_2.bias torch.Size([256])
transformer.h.2.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.2.mlp.c_fc.bias torch.Size([1024])
transformer.h.2.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.2.mlp.c_proj.bias torch.Size([256])
transformer.h.3.ln_1.weight torch.Size([256])
transformer.h.3.ln_1.bias torch.Size([256])
transformer.h.3.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.3.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.3.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.3.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.3.attn.attention.out_proj.bias torch.Size([256])
transformer.h.3.ln_2.weight torch.Size([256])
transformer.h.3.ln_2.bias torch.Size([256])
transformer.h.3.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.3.mlp.c_fc.bias torch.Size([1024])
transformer.h.3.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.3.mlp.c_proj.bias torch.Size([256])
transformer.h.4.ln_1.weight torch.Size([256])
transformer.h.4.ln_1.bias torch.Size([256])
transformer.h.4.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.4.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.4.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.4.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.4.attn.attention.out_proj.bias torch.Size([256])
transformer.h.4.ln_2.weight torch.Size([256])
transformer.h.4.ln_2.bias torch.Size([256])
transformer.h.4.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.4.mlp.c_fc.bias torch.Size([1024])
transformer.h.4.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.4.mlp.c_proj.bias torch.Size([256])
transformer.h.5.ln_1.weight torch.Size([256])
transformer.h.5.ln_1.bias torch.Size([256])
transformer.h.5.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.5.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.5.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.5.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.5.attn.attention.out_proj.bias torch.Size([256])
transformer.h.5.ln_2.weight torch.Size([256])
transformer.h.5.ln_2.bias torch.Size([256])
transformer.h.5.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.5.mlp.c_fc.bias torch.Size([1024])
transformer.h.5.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.5.mlp.c_proj.bias torch.Size([256])
transformer.h.6.ln_1.weight torch.Size([256])
transformer.h.6.ln_1.bias torch.Size([256])
transformer.h.6.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.6.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.6.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.6.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.6.attn.attention.out_proj.bias torch.Size([256])
transformer.h.6.ln_2.weight torch.Size([256])
transformer.h.6.ln_2.bias torch.Size([256])
transformer.h.6.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.6.mlp.c_fc.bias torch.Size([1024])
transformer.h.6.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.6.mlp.c_proj.bias torch.Size([256])
transformer.h.7.ln_1.weight torch.Size([256])
transformer.h.7.ln_1.bias torch.Size([256])
transformer.h.7.attn.attention.k_proj.weight torch.Size([256, 256])
transformer.h.7.attn.attention.v_proj.weight torch.Size([256, 256])
transformer.h.7.attn.attention.q_proj.weight torch.Size([256, 256])
transformer.h.7.attn.attention.out_proj.weight torch.Size([256, 256])
transformer.h.7.attn.attention.out_proj.bias torch.Size([256])
transformer.h.7.ln_2.weight torch.Size([256])
transformer.h.7.ln_2.bias torch.Size([256])
transformer.h.7.mlp.c_fc.weight torch.Size([1024, 256])
transformer.h.7.mlp.c_fc.bias torch.Size([1024])
transformer.h.7.mlp.c_proj.weight torch.Size([256, 1024])
transformer.h.7.mlp.c_proj.bias torch.Size([256])
transformer.ln_f.weight torch.Size([256])
transformer.ln_f.bias torch.Size([256])
lm_head.weight torch.Size([50257, 256])