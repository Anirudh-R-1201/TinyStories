{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9739960,"sourceType":"datasetVersion","datasetId":5961698}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet datasets wandb","metadata":{"id":"ecFWX7ztIKKj","execution":{"iopub.status.busy":"2024-10-29T23:46:35.148817Z","iopub.execute_input":"2024-10-29T23:46:35.149829Z","iopub.status.idle":"2024-10-29T23:46:48.880551Z","shell.execute_reply.started":"2024-10-29T23:46:35.149769Z","shell.execute_reply":"2024-10-29T23:46:48.879214Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom transformers import AutoTokenizer, GPTNeoConfig, GPTNeoForCausalLM\nimport json\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nimport wandb","metadata":{"id":"Efgn_1TPIhvU","execution":{"iopub.status.busy":"2024-10-29T23:46:48.882874Z","iopub.execute_input":"2024-10-29T23:46:48.883501Z","iopub.status.idle":"2024-10-29T23:46:55.923045Z","shell.execute_reply.started":"2024-10-29T23:46:48.883463Z","shell.execute_reply":"2024-10-29T23:46:55.922090Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"configdict = { \n    \"gpt-20M\": { \n        \"batch_size\": 32,\n        \"block_size\": 256,\n        \"lr\": 5e-4,\n        \"n_layer\": 6,\n        \"n_head\": 6,\n        \"n_embd\": 288,\n        \"dropout\": 0.1,\n        \"weight_decay\": 0.01,\n        \"epochs\": 1,\n        \"eval_interval\": 200,\n        \"eval_steps\": 50,\n        \"vocab_size\": 50257, #8000\n        \"warmup_tokens\": 10000,\n        \"gradient_accumulation_steps\": 8,\n    },\n    \"gpt-8M\": {\n        \"batch_size\": 64,\n        \"block_size\": 128,\n        \"lr\": 5e-4,\n        \"n_layer\": 8,\n        \"n_head\": 8,\n        \"n_embd\": 128,\n        \"dropout\": 0.1,\n        \"weight_decay\": 0.01,\n        \"epochs\": 1,\n        \"eval_interval\": 200,\n        \"eval_steps\": 50,\n        \"vocab_size\": 50257, #8000\n        \"warmup_tokens\": 10000,\n        \"gradient_accumulation_steps\": 16,\n    },\n    \"gpt-3M\": {\n        \"batch_size\": 64,\n        \"block_size\": 128,\n        \"lr\": 6e-4,\n        \"n_layer\": 4,\n        \"n_head\": 4,\n        \"n_embd\": 64,\n        \"dropout\": 0.1,\n        \"weight_decay\": 0.01,\n        \"epochs\": 1,\n        \"eval_interval\": 200,\n        \"eval_steps\": 50,\n        \"vocab_size\": 50257,\n        \"warmup_tokens\": 5000,\n        \"gradient_accumulation_steps\": 16,\n    },\n    \"tokenizer\": {\n        \"name\": \"EleutherAI/gpt-neo-125M\",\n    },\n    \"data\": {\n        \"name\": \"roneneldan/TinyStories\",\n    },\n}","metadata":{"id":"gdh9KJDBInuE","execution":{"iopub.status.busy":"2024-10-29T23:46:55.924153Z","iopub.execute_input":"2024-10-29T23:46:55.924437Z","iopub.status.idle":"2024-10-29T23:46:55.935850Z","shell.execute_reply.started":"2024-10-29T23:46:55.924403Z","shell.execute_reply":"2024-10-29T23:46:55.934700Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Config:\n    def __init__(self, dictionary):\n        for key, value in dictionary.items():\n            if isinstance(value, dict):\n                setattr(self, key, Config(value))\n            else:\n                setattr(self, key, value)\n\n    def __getitem__(self, key):\n        return self.__dict__[key]\n\nconfig = Config(configdict)","metadata":{"id":"HLhq4lgCIpv_","execution":{"iopub.status.busy":"2024-10-29T23:46:55.938616Z","iopub.execute_input":"2024-10-29T23:46:55.939019Z","iopub.status.idle":"2024-10-29T23:46:55.946608Z","shell.execute_reply.started":"2024-10-29T23:46:55.938975Z","shell.execute_reply":"2024-10-29T23:46:55.945672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"oVRef_73IrHw","execution":{"iopub.status.busy":"2024-10-29T23:46:55.947548Z","iopub.execute_input":"2024-10-29T23:46:55.947880Z","iopub.status.idle":"2024-10-29T23:46:55.956199Z","shell.execute_reply.started":"2024-10-29T23:46:55.947849Z","shell.execute_reply":"2024-10-29T23:46:55.955291Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"###If running on Colab\n#import os\n\n#my_secret = os.getenv(\"WANDB_API_KEY\")\n\n#wandb_config = {k: v for k, v in vars(config).items() if not callable(getattr(config, k)) and not k.startswith(\"__\")}  # Creating Wandb hyperparameters config for tracking experiments\n\n#wandb.login(key=my_secret)\n#run = wandb.init(project=\"gptneo-tinystories\", name=\"tinystories-8M-3\", config=wandb_config)\n#wandb_config","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"v8cYHE9KIsgr","outputId":"cc73b041-2a10-47a2-e86f-04bf36b38317","execution":{"iopub.status.busy":"2024-10-29T23:46:55.957383Z","iopub.execute_input":"2024-10-29T23:46:55.957744Z","iopub.status.idle":"2024-10-29T23:46:55.971262Z","shell.execute_reply.started":"2024-10-29T23:46:55.957702Z","shell.execute_reply":"2024-10-29T23:46:55.970276Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nsecret_label = \"WANDB_API_KEY\"\nmy_secret = UserSecretsClient().get_secret(secret_label)\n\nwandb_config = {k: v for k, v in vars(config).items() if not callable(getattr(config, k)) and not k.startswith(\"__\")}  # Creating Wandb hyperparameters config for tracking experiments\n\nwandb.login(key=my_secret)\nrun = wandb.init(project=\"gptneo-tinystories\", name=\"tinystories20M-5\", config=wandb_config)\nwandb_config","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:46:55.972684Z","iopub.execute_input":"2024-10-29T23:46:55.973020Z","iopub.status.idle":"2024-10-29T23:47:00.588116Z","shell.execute_reply.started":"2024-10-29T23:46:55.972981Z","shell.execute_reply":"2024-10-29T23:47:00.587158Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manirudhr1201\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113734699999137, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f64dc4060144b2bad2994a655c880ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241029_234657-u33urp5p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/anirudhr1201/gptneo-tinystories--/runs/u33urp5p' target=\"_blank\">tinystories3M-4</a></strong> to <a href='https://wandb.ai/anirudhr1201/gptneo-tinystories--' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/anirudhr1201/gptneo-tinystories--' target=\"_blank\">https://wandb.ai/anirudhr1201/gptneo-tinystories--</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/anirudhr1201/gptneo-tinystories--/runs/u33urp5p' target=\"_blank\">https://wandb.ai/anirudhr1201/gptneo-tinystories--/runs/u33urp5p</a>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'gpt-20M': <__main__.Config at 0x7e5256dcb310>,\n 'gpt-8M': <__main__.Config at 0x7e5256dcb670>,\n 'gpt-3M': <__main__.Config at 0x7e5256dcb700>,\n 'tokenizer': <__main__.Config at 0x7e5256dcb6d0>,\n 'data': <__main__.Config at 0x7e5256dcb2e0>}"},"metadata":{}}]},{"cell_type":"code","source":"class TokenizerOLD:\n    def __init__(self, config, device=\"cuda\"):\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(config.name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.vocab_size = self.tokenizer.vocab_size\n\n    def encoder(self, input, padding=True, max_length=256, truncation=True):\n        #return self.tokenizer(input, return_tensors='pt', padding=padding, max_length=max_length, truncation=truncation)['input_ids'].to(self.device)\n        encoded = self.tokenizer(input, return_tensors='pt', padding=padding, max_length=max_length, truncation=truncation)\n        return {\n            'input_ids': encoded['input_ids'].to(self.device),\n            'attention_mask': encoded['attention_mask'].to(self.device)\n        }\n    def decoder(self, tokens):\n        return self.tokenizer.decode(tokens[0], skip_special_tokens=True)\n","metadata":{"id":"D7ArY1OkI19t","execution":{"iopub.status.busy":"2024-10-29T23:47:00.589229Z","iopub.execute_input":"2024-10-29T23:47:00.589523Z","iopub.status.idle":"2024-10-29T23:47:00.597755Z","shell.execute_reply.started":"2024-10-29T23:47:00.589488Z","shell.execute_reply":"2024-10-29T23:47:00.596823Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class Tokenizer:\n    def __init__(self, config, k=None, file_path=\"None\", device=\"cuda\"):\n        self.k = k\n        self.file_path = file_path\n        self.device = device\n        self.tokenizer = AutoTokenizer.from_pretrained(config.name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n        self.vocab_size = self.tokenizer.vocab_size if not self.k else self.k\n        self.initialize()\n\n    def get_config(self):\n        config = {\n            \"initl_vocab_size\": self.tokenizer.vocab_size,\n            \"final_vocab_size\": self.vocab_size,\n            \"vocab_size\": self.vocab_size,\n            \"total_tokens\": self.total_tokens,\n            \"total_tokens_used\": self.tokens_used if self.k else self.total_tokens,\n            \"total_unsed_tokens\": self.total_tokens - self.tokens_used if self.k else 0\n        }\n        return config\n\n    def initialize(self):\n        with open(self.file_path, 'r') as file:\n            tokens_counts = json.load(file)\n\n        self.total_tokens = sum(tokens_counts.values())  # Already sorted\n\n        if self.k:\n            self.tokens_used = sum([i for i in tokens_counts.values()][:self.k])\n            self.top_k_tokens = [i for i in tokens_counts.keys()][:self.k]  # We will only use top k tokens, others will be ignored\n            self.top_k_tokens.append(\"50256\")\n            self.vocab_size += 1\n            self.top_k_tokens_dict = {token: index for index, token in enumerate(self.top_k_tokens)}\n            self.reversed_top_k_tokens_dict = {value: int(key) for key, value in self.top_k_tokens_dict.items()}\n\n    def encoder(self, input, padding=False, max_length=512, truncation=False):\n        tokens = self.tokenizer(input, return_tensors='pt', padding=padding, max_length=max_length, truncation=truncation)['input_ids'].to(self.device)\n\n        if self.k:\n            tokens = torch.tensor([self.top_k_tokens_dict.get(str(token.item()), self.top_k_tokens_dict[\"50256\"]) for token in tokens.view(-1)], device=self.device).view(tokens.shape)\n\n        return tokens\n    \n    def decoder(self, tokens):\n        if self.k:\n            tokens = torch.tensor([[self.reversed_top_k_tokens_dict.get(token.item(), self.reversed_top_k_tokens_dict[self.vocab_size - 1]) for token in row] for row in tokens], device=tokens.device)\n\n        output = [self.tokenizer.decode(x, skip_special_tokens=True) for x in tokens]\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:47:00.598980Z","iopub.execute_input":"2024-10-29T23:47:00.599263Z","iopub.status.idle":"2024-10-29T23:47:00.615892Z","shell.execute_reply.started":"2024-10-29T23:47:00.599232Z","shell.execute_reply":"2024-10-29T23:47:00.614912Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, config, device='cuda'):\n        super().__init__()\n        self.device = device\n        self.block_size = config.block_size\n\n        self.gpt_neo_config = GPTNeoConfig(\n            vocab_size=config.vocab_size,\n            max_position_embeddings=config.block_size,\n            hidden_size=config.n_embd,\n            num_layers=config.n_layer,\n            num_heads=config.n_head,\n            attention_types=[[[\"global\", \"local\"], config.n_layer//2]],\n            intermediate_size=config.n_embd * 4,\n            activation_function=\"gelu_new\",\n            resid_dropout=config.dropout,\n            embed_dropout=config.dropout,\n            attention_dropout=config.dropout,\n            layer_norm_epsilon=1e-5,\n            initializer_range=0.02,\n            use_cache=True,\n            pad_token_id=50256,  \n            eos_token_id=50256\n        )\n\n        self.model = GPTNeoForCausalLM(self.gpt_neo_config).to(device)\n    \n    def get_config(self):\n        return self.gpt_neo_config\n\n    def get_parameters(self):\n        return sum(p.numel() for p in self.parameters())\n\n    def save(self, path):\n        torch.save(self.model.state_dict(), path)\n\n    def forward(self, idx, targets=None):\n        try:\n            outputs = self.model(idx, labels=targets)\n            return outputs.logits, outputs.loss\n        except RuntimeError as e:\n            print(f\"Error in forward pass: {str(e)}\")\n            print(f\"Input shape: {idx.shape}\")\n            print(f\"Target shape: {targets.shape if targets is not None else 'None'}\")\n            raise\n\n    def generate(self, idx, max_tokens, temperature=0.3, top_k=None):\n        gen_kwargs = {\n            \"max_length\": idx.shape[1] + max_tokens,\n            \"temperature\": temperature,\n            \"top_k\": top_k,\n            \"do_sample\": True,\n            \"pad_token_id\": self.model.config.pad_token_id,\n        }\n\n        generated = self.model.generate(idx, **gen_kwargs)\n        return generated","metadata":{"id":"-EUieZFiJA4P","execution":{"iopub.status.busy":"2024-10-29T23:47:00.619226Z","iopub.execute_input":"2024-10-29T23:47:00.619522Z","iopub.status.idle":"2024-10-29T23:47:00.633284Z","shell.execute_reply.started":"2024-10-29T23:47:00.619490Z","shell.execute_reply":"2024-10-29T23:47:00.632379Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def load_data(config, batch_size, device='cuda'):\n    dataset = load_dataset(config.name)\n    generator = torch.Generator(device=device)\n    generator.manual_seed(42)\n    train_data = DataLoader(dataset[\"train\"][\"text\"], batch_size=batch_size, shuffle=True, pin_memory=True, generator=generator)\n    val_data = DataLoader(dataset[\"validation\"][\"text\"], batch_size=batch_size, shuffle=True, pin_memory=True, generator=generator)\n    return train_data, val_data","metadata":{"id":"mTs0jKnNJLF3","execution":{"iopub.status.busy":"2024-10-29T23:47:00.634392Z","iopub.execute_input":"2024-10-29T23:47:00.634707Z","iopub.status.idle":"2024-10-29T23:47:00.647958Z","shell.execute_reply.started":"2024-10-29T23:47:00.634667Z","shell.execute_reply":"2024-10-29T23:47:00.647039Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss(model, train_data, val_data, encoder, eval_steps=50):\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_steps)\n        for k in range(eval_steps):\n            data = train_data if split == 'train' else val_data\n            tokens = encoder(next(iter(data)), max_length=model.block_size, padding=\"max_length\", truncation=True)\n            _, loss = model(tokens, tokens)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"id":"y1iwZxCiJMWq","execution":{"iopub.status.busy":"2024-10-29T23:47:00.649063Z","iopub.execute_input":"2024-10-29T23:47:00.649388Z","iopub.status.idle":"2024-10-29T23:47:00.662583Z","shell.execute_reply.started":"2024-10-29T23:47:00.649355Z","shell.execute_reply":"2024-10-29T23:47:00.661649Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, config, model, optimizer, train_data, val_data, encoder, scaler, device='cuda', scheduler=None):\n        self.config = config\n        self.model = model.to(device)\n        self.optimizer = optimizer\n        self.train_data = train_data\n        self.val_data = val_data\n        self.encoder = encoder\n        self.scheduler = scheduler\n        self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else scaler\n        self.device = device\n        self.best_val_loss = float('inf')\n        self.latest_best_model = \"\"\n        self.gradient_accumulation_steps = config.gradient_accumulation_steps\n\n    def trainOLD(self, epochs, eval_interval=200, eval_steps=50):\n        for epoch in range(epochs):\n            self.model.train()\n            for i, batch in enumerate(self.train_data):\n                tokens = self.encoder(batch, max_length=self.config.block_size, padding=\"max_length\", truncation=True)\n                _, loss = self.model(tokens, tokens)\n                print(f\"Epoch: {epoch + 1}/{epochs} | Batch: {i + 1}/{len(self.train_data)} | Loss: {loss.item():.4f}\")\n\n                # Normalize the loss to account for gradient accumulation\n                loss = loss / self.gradient_accumulation_steps\n                loss.backward()\n\n                if (i + 1) % self.gradient_accumulation_steps == 0:\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n                    if self.scheduler:\n                        self.scheduler.step()\n\n            if (epoch + 1) % eval_interval == 0 or epoch == epochs - 1:\n                losses = estimate_loss(self.model, self.train_data, self.val_data, self.encoder, eval_steps)\n                print(f\"Epoch: {epoch + 1}/{epochs} | Train loss: {losses['train']:.4f} | Val loss: {losses['val']:.4f}\")\n                wandb.log({\n                    \"epoch\": epoch,\n                    \"train/loss\": losses['train'],\n                    \"val/loss\": losses['val'],\n                    \"lr\": self.optimizer.param_groups[0]['lr']\n                })\n                if losses['val'] < self.best_val_loss:\n                    self.best_val_loss = losses['val']\n                    torch.save(self.model.state_dict(), f\"checkpoint_epoch_{epoch+1}.pt\")\n                    self.latest_best_model = f\"checkpoint_epoch_{epoch+1}.pt\"\n\n        return self.latest_best_model\n\n    def train(self, epochs, eval_interval=200, eval_steps=50, generator=None):\n        max_steps = epochs * len(self.train_data)\n        steps = 0\n        tracked_losses = []\n\n        for epoch in range(epochs):\n            self.model.train()\n            for batch_idx, batch in enumerate(self.train_data):\n                tokens = self.encoder(batch, max_length=self.config.block_size, padding=\"max_length\", truncation=True)\n                #tokens = {k: v.to(self.device) for k, v in tokens.items()}#tokens.to(self.device)\n\n                with torch.amp.autocast('cuda', dtype=torch.float16):\n                    _, output_loss = self.model(tokens, tokens)\n                    loss = output_loss / self.gradient_accumulation_steps\n\n                self.scaler.scale(loss).backward()\n\n                if (batch_idx + 1) % self.gradient_accumulation_steps == 0 or (batch_idx + 1) == len(self.train_data):\n                    self.scaler.unscale_(self.optimizer)\n                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n                    self.scaler.step(self.optimizer)\n                    self.scaler.update()\n                    self.optimizer.zero_grad(set_to_none=True)\n                    if self.scheduler:\n                        self.scheduler.step()\n                    steps += 1\n\n                    if steps % eval_interval == 0 or steps == max_steps-1:\n                        print(f\"Starting Epoch: {epoch + 1} {'-' * 100}\")\n                        losses = estimate_loss(self.model, self.train_data, self.val_data, self.encoder, eval_steps)\n                        tracked_losses.append(losses)\n                        print(f\"Epoch: {epoch + 1}/{epochs} | Step: {steps} | Train loss: {losses['train']:.4f} | Val loss: {losses['val']:.4f}\")\n                        wandb.log({\n                            \"epoch\": epoch,\n                            \"train/loss\": losses['train'],\n                            \"val/loss\": losses['val'],\n                            \"lr\": self.optimizer.param_groups[0]['lr']\n                        })\n                        if losses['val'] < self.best_val_loss:\n                            self.best_val_loss = losses['val']\n                            torch.save(self.model.state_dict(), f\"checkpoint_epoch_{epoch+1}.pt\")\n                            self.latest_best_model = f\"checkpoint_epoch_{epoch+1}.pt\"\n\n                        else:\n                            print(\"Validation loss did not improve, consider stopping early.\")\n\n        return self.latest_best_model\n","metadata":{"id":"FM_c7D1kJNbb","execution":{"iopub.status.busy":"2024-10-29T23:47:00.664103Z","iopub.execute_input":"2024-10-29T23:47:00.664381Z","iopub.status.idle":"2024-10-29T23:47:00.689324Z","shell.execute_reply.started":"2024-10-29T23:47:00.664350Z","shell.execute_reply":"2024-10-29T23:47:00.688456Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\ntorch.set_default_device(device)\n\nmodel_name = \"gpt-20M\"\nmodel_config = config[model_name]","metadata":{"id":"afYDStD7JPdc","execution":{"iopub.status.busy":"2024-10-29T23:47:00.690383Z","iopub.execute_input":"2024-10-29T23:47:00.690759Z","iopub.status.idle":"2024-10-29T23:47:00.725821Z","shell.execute_reply.started":"2024-10-29T23:47:00.690716Z","shell.execute_reply":"2024-10-29T23:47:00.724864Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(config.tokenizer, file_path=\"/kaggle/input/tokengptneo/tokens.json\", device=device)\ntrain_data, val_data = load_data(config.data, model_config.batch_size, device=device)","metadata":{"id":"4WfxaAAmJQpu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7f6199dd-c385-455e-930a-dd76123de358","execution":{"iopub.status.busy":"2024-10-29T23:47:00.727008Z","iopub.execute_input":"2024-10-29T23:47:00.727271Z","iopub.status.idle":"2024-10-29T23:47:21.257443Z","shell.execute_reply.started":"2024-10-29T23:47:00.727241Z","shell.execute_reply":"2024-10-29T23:47:21.256147Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f4e28e5d5f43de87d8b5d5f0c63645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bca660ac772c4be2a6db9fd04b34d870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca1393e6902741109701a516e7d92e93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2572d6f2f84a0b8e9120a75e25c875"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dcf516a5b0140a786fc9053a9907fa0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc0a5bb820c7447db033d8d1a1a41606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00004-2d5a1467fff1081b.parquet:   0%|          | 0.00/249M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d09bf3999d4180bdf4e6ac0700fccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00001-of-00004-5852b56a2bd28fd9.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9980ace65c44a769d22f88362ac332e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00002-of-00004-a26307300439e943.parquet:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1df89e2fc38a4bb5ba294f422ac8d6c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00003-of-00004-d243063613e5a057.parquet:   0%|          | 0.00/248M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b6fa3ca4426400580f48ca47a619bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)-00000-of-00001-869c898b519ad725.parquet:   0%|          | 0.00/9.99M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ec134a25bf84444bca38db9f6654335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2119719 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af26cbd0e5149219fefd97dae296c80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/21990 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df7919cd06dc42dc9401b51595832905"}},"metadata":{}}]},{"cell_type":"code","source":"model = GPTModel(model_config, device=device)\nprint(f\"Model has {model.get_parameters():,} parameters\")\nmodel = model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvV_LZfIJSzN","outputId":"5a88f7ca-da53-4b0b-e4b4-2ae40ff58075","execution":{"iopub.status.busy":"2024-10-29T23:47:21.259249Z","iopub.execute_input":"2024-10-29T23:47:21.259904Z","iopub.status.idle":"2024-10-29T23:47:21.668751Z","shell.execute_reply.started":"2024-10-29T23:47:21.259865Z","shell.execute_reply":"2024-10-29T23:47:21.667615Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model has 3,423,936 parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"optim = torch.optim.AdamW(model.parameters(), lr=model_config.lr, weight_decay=model_config.weight_decay)\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nscaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))","metadata":{"id":"wnhJ0GGzJcT8","execution":{"iopub.status.busy":"2024-10-29T23:47:21.670573Z","iopub.execute_input":"2024-10-29T23:47:21.670981Z","iopub.status.idle":"2024-10-29T23:47:22.207821Z","shell.execute_reply.started":"2024-10-29T23:47:21.670937Z","shell.execute_reply":"2024-10-29T23:47:22.206751Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model_config, model, optim, train_data, val_data, tokenizer.encoder, scaler, device)\nbestmodel = trainer.train(epochs=model_config.epochs, eval_interval=model_config.eval_interval, eval_steps=model_config.eval_steps)","metadata":{"id":"tfRADpMJJ3f7","execution":{"iopub.status.busy":"2024-10-29T23:47:22.209565Z","iopub.execute_input":"2024-10-29T23:47:22.210058Z","iopub.status.idle":"2024-10-29T23:51:04.370885Z","shell.execute_reply.started":"2024-10-29T23:47:22.209999Z","shell.execute_reply":"2024-10-29T23:51:04.369389Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/4227591036.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler() if device == 'cuda' else scaler\n","output_type":"stream"},{"name":"stdout","text":"Starting Epoch: 1 ----------------------------------------------------------------------------------------------------\nEpoch: 1/1 | Step: 200 | Train loss: 5.1727 | Val loss: 5.1601\nStarting Epoch: 1 ----------------------------------------------------------------------------------------------------\nEpoch: 1/1 | Step: 400 | Train loss: 3.9857 | Val loss: 3.9825\nStarting Epoch: 1 ----------------------------------------------------------------------------------------------------\nEpoch: 1/1 | Step: 600 | Train loss: 3.5977 | Val loss: 3.5710\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model_config, model, optim, train_data, val_data, tokenizer\u001b[38;5;241m.\u001b[39mencoder, scaler, device)\n\u001b[0;32m----> 2\u001b[0m bestmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_steps\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[13], line 59\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, epochs, eval_interval, eval_steps, generator)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_data):\n\u001b[0;32m---> 59\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m#tokens = {k: v.to(self.device) for k, v in tokens.items()}#tokens.to(self.device)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n","Cell \u001b[0;32mIn[9], line 37\u001b[0m, in \u001b[0;36mTokenizer.encoder\u001b[0;34m(self, input, padding, max_length, truncation)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 37\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk:\n\u001b[1;32m     40\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k_tokens_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mstr\u001b[39m(token\u001b[38;5;241m.\u001b[39mitem()), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k_tokens_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50256\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(tokens\u001b[38;5;241m.\u001b[39mshape)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 3024\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3026\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3112\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3108\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3109\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3110\u001b[0m         )\n\u001b[1;32m   3111\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 3112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3114\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   3135\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   3136\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3155\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3314\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3304\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3305\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3306\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3307\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3311\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3312\u001b[0m )\n\u001b[0;32m-> 3314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3316\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3332\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3333\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3334\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py:127\u001b[0m, in \u001b[0;36mGPT2TokenizerFast._batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m is_split_into_words \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_split_into_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_split_into_words, (\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with add_prefix_space=True \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto use it with pretokenized inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m )\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:577\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_ids \u001b[38;5;129;01min\u001b[39;00m sanitized_tokens[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)\n\u001b[0;32m--> 577\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msanitized_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanitized_encodings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    772\u001b[0m     value \u001b[38;5;241m=\u001b[39m [value]\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m[key] \u001b[38;5;241m=\u001b[39m tensor\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#bestmodel = f\"checkpoint_epoch_1.pt\"\ncheckpoint = torch.load(bestmodel)\nmodel.load_state_dict(checkpoint) \nmodel.eval()","metadata":{"id":"CXXF5CKvKB7l","execution":{"iopub.status.busy":"2024-10-29T23:51:18.028091Z","iopub.execute_input":"2024-10-29T23:51:18.028745Z","iopub.status.idle":"2024-10-29T23:51:18.091401Z","shell.execute_reply.started":"2024-10-29T23:51:18.028703Z","shell.execute_reply":"2024-10-29T23:51:18.090432Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1746287869.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(bestmodel)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"GPTModel(\n  (model): GPTNeoForCausalLM(\n    (transformer): GPTNeoModel(\n      (wte): Embedding(50257, 64)\n      (wpe): Embedding(128, 64)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-3): 4 x GPTNeoBlock(\n          (ln_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (attn): GPTNeoAttention(\n            (attention): GPTNeoSelfAttention(\n              (attn_dropout): Dropout(p=0.1, inplace=False)\n              (resid_dropout): Dropout(p=0.1, inplace=False)\n              (k_proj): Linear(in_features=64, out_features=64, bias=False)\n              (v_proj): Linear(in_features=64, out_features=64, bias=False)\n              (q_proj): Linear(in_features=64, out_features=64, bias=False)\n              (out_proj): Linear(in_features=64, out_features=64, bias=True)\n            )\n          )\n          (ln_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPTNeoMLP(\n            (c_fc): Linear(in_features=64, out_features=256, bias=True)\n            (c_proj): Linear(in_features=256, out_features=64, bias=True)\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=64, out_features=50257, bias=False)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"#torch.save(model.state_dict(), 'model.bin')\n#model.eval()","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:51:04.376948Z","iopub.status.idle":"2024-10-29T23:51:04.377446Z","shell.execute_reply.started":"2024-10-29T23:51:04.377177Z","shell.execute_reply":"2024-10-29T23:51:04.377203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\n\ndef load_model(config, path, device='cuda'):\n    model = GPTModel(config, device=device)\n    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n    model.to(device)\n    model.eval()\n    return model\n\ndef clean_string(input_string):\n    print(input_string[0])\n    cleaned_string = re.sub(r'[^\\w\\s.,]', '', input_string[0])\n    cleaned_string = cleaned_string.replace('\\n', '')\n    return cleaned_string","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:51:21.223825Z","iopub.execute_input":"2024-10-29T23:51:21.224218Z","iopub.status.idle":"2024-10-29T23:51:21.232674Z","shell.execute_reply.started":"2024-10-29T23:51:21.224181Z","shell.execute_reply":"2024-10-29T23:51:21.231766Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#prompt =\"'Hi Jane, have you seen Alice? I can't find her anywhere', said Jack\"\n#outputs = {}\n#model.eval()\n#for t in range(1,11):\n#    for k in range(15):\n#        if k==0:\n#            output = model.generate(tokenizer.encoder(prompt), max_tokens=150, temperature=float(t/10), top_k=None)\n#        else:\n#            output = model.generate(tokenizer.encoder(prompt), max_tokens=150, temperature=float(t/10), top_k=k)\n        #try:\n#        outputs[(t/10,k)] = clean_string(tokenizer.decoder(output))\n        #except:\n        #outputs[(t/10,k)] = (tokenizer.decoder(output2))\n#print(outputs)","metadata":{"id":"xkpUAvekKD6U","execution":{"iopub.status.busy":"2024-10-29T23:51:21.949878Z","iopub.execute_input":"2024-10-29T23:51:21.950799Z","iopub.status.idle":"2024-10-29T23:51:21.955738Z","shell.execute_reply.started":"2024-10-29T23:51:21.950757Z","shell.execute_reply":"2024-10-29T23:51:21.954664Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"prompt =\"'Hi Jane, have you seen Alice? I can't find her anywhere', said Jack\"\noutput = model.generate(tokenizer.encoder(prompt), max_tokens=100, temperature=0.2, top_k=None)\nprint(clean_string(tokenizer.decoder(output)))","metadata":{"execution":{"iopub.status.busy":"2024-10-29T23:51:23.769156Z","iopub.execute_input":"2024-10-29T23:51:23.769549Z","iopub.status.idle":"2024-10-29T23:51:24.808425Z","shell.execute_reply.started":"2024-10-29T23:51:23.769511Z","shell.execute_reply":"2024-10-29T23:51:24.807422Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"'Hi Jane, have you seen Alice? I can't find her anywhere', said Jack.\nOne day, Lily's mommy's mommy's mommy's mommy and said, \"I can go to the park. I have to the park!\"\n\nLily said, \"I'm very happy. \"I want to play with the park. I have to go to the park. I want to the park. \"I want to play with you to play with you to play with you!\"\n\n\"Look, I can play with the park.\n\n\nHi Jane, have you seen Alice I cant find her anywhere, said Jack.One day, Lilys mommys mommys mommys mommy and said, I can go to the park. I have to the parkLily said, Im very happy. I want to play with the park. I have to go to the park. I want to the park. I want to play with you to play with you to play with youLook, I can play with the park.\n","output_type":"stream"}]},{"cell_type":"code","source":"#import os\n#from huggingface_hub import HfApi, Repository\n#from kaggle_secrets import UserSecretsClient\n\n#def save_to_huggingface(model, config, tokenizer, repo_name, commit_message=\"Update model\"):\n#    temp_dir = \"/kaggle/working/\"\n#    os.makedirs(temp_dir, exist_ok=True)\n\n#    model_path = os.path.join(\"/\", \"model.bin\")\n#    torch.save(model.state_dict(), 'pytorch_model.bin') \n    \n#    config_dict = config.to_dict()\n#    config_path = os.path.join(temp_dir, \"config.json\")\n#    with open(config_path, 'w') as f:\n#        print (config)\n#        json.dump(config_dict, f, indent=2)\n\n#    #tokenizer.tokenizer.save_pretrained(temp_dir)\n\n#    # Get Hugging Face token from Kaggle secrets\n#    user_secrets = UserSecretsClient()\n#    hf_token = user_secrets.get_secret(\"huggingface_token\")\n\n#    # Initialize HfApi with the token\n#    api = HfApi(token=hf_token)\n    \n#    api.upload_file(\n#        path_or_fileobj=\"pytorch_model.bin\",\n#        path_in_repo=\"pytorch_model.bin\",\n#        repo_id=repo_id,\n#        repo_type=\"model\",\n#    )\n    \n\n#    api.upload_file(\n#        path_or_fileobj=\"config.json\",\n#        path_in_repo=\"config.json\",\n#        repo_id=repo_id,\n#    )\n    \n#    api.upload_file(\n#        path_or_fileobj=\"/kaggle/input/tokengptneo/tokens.json\",\n#        path_in_repo=\"tokens.json\",\n#        repo_id=repo_id,\n#    )\n    \n\n\n#    #repo_url = api.create_repo(repo_name, exist_ok=True)\n#    #repo = Repository(local_dir=\"/\", clone_from=repo_url)\n\n#    #repo.git_add()\n#    #repo.git_commit(commit_message)\n#    #repo.git_push()\n\n#    #print(f\"Model, config, and tokenizer saved to: {repo_url}\")\n\n#    #for file in os.listdir(temp_dir):\n#    #    os.remove(os.path.join(temp_dir, file))\n#    #os.rmdir(temp_dir)\n\n    \n    \n#repo_id = \"AnirudhRajagopalan1201/tinystories-custom-3M\"\n#model_config = model.get_config()\n#model_config.save_pretrained('./')\n#save_to_huggingface(model, model_config, None, repo_id)\n","metadata":{"id":"gRtFYbdAKHgf","execution":{"iopub.status.busy":"2024-10-29T21:53:41.987125Z","iopub.status.idle":"2024-10-29T21:53:41.987674Z","shell.execute_reply.started":"2024-10-29T21:53:41.987486Z","shell.execute_reply":"2024-10-29T21:53:41.987506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login, login\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"huggingface_token\")\nlogin(token=hf_token)\nrepo =\"AnirudhRajagopalan1201/tinystories-custom-20M\" \nmodel.model.save_pretrained(\"./\")\nmodel.model.push_to_hub(repo)\ntokenizer.tokenizer.save_pretrained(\"./\")\ntokenizer.tokenizer.push_to_hub(repo)","metadata":{"id":"PmwbUf1tObxJ","execution":{"iopub.status.busy":"2024-10-29T23:52:16.376093Z","iopub.execute_input":"2024-10-29T23:52:16.376986Z","iopub.status.idle":"2024-10-29T23:52:20.544419Z","shell.execute_reply.started":"2024-10-29T23:52:16.376941Z","shell.execute_reply":"2024-10-29T23:52:20.543525Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/13.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8522ea8dc28849b2b3393ffd97116a4c"}},"metadata":{}},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/AnirudhRajagopalan1201/tinystories-custom-3M/commit/c5dfcbfb523ecbfdbf8cec9dbaeede806698fc36', commit_message='Upload tokenizer', commit_description='', oid='c5dfcbfb523ecbfdbf8cec9dbaeede806698fc36', pr_url=None, repo_url=RepoUrl('https://huggingface.co/AnirudhRajagopalan1201/tinystories-custom-3M', endpoint='https://huggingface.co', repo_type='model', repo_id='AnirudhRajagopalan1201/tinystories-custom-3M'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}